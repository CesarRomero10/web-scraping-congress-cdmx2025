{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPLGK3ecXyrPVbfxDnGUSGP"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Proyecto Módilo I: Web Scraping**\n",
        "\n",
        "## **Diplomado en Ciencia de Datos, UNAM 2025**\n",
        "\n",
        "#### **Autor: Mtro. José César Romero Galván**\n",
        "\n",
        "Proyecto de Web Scraping sobre el número de iniciativas presentadas en el Congreso de la Ciudad de México en el año legislativo 2024 - 2025 de la III Legislatura.\n",
        "\n",
        "### **Objetivos:**\n",
        "\n",
        "Extracción: Nombre de la iniciativa, nombre del suscriptor o suscriptora, partido político, institución, asociación y URL\n",
        "\n",
        "Salida: Iniciativas_CDMX_2024_2025.CSV\n",
        "\n"
      ],
      "metadata": {
        "id": "6IhEbZWpW_4Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Instalamos BeautifulSoap (Ya viene instalado en colab)\n",
        "\n",
        "! pip install beautifulsoup4\n",
        "\n",
        "# \"!\" se usa en entornos de shell (como Bash o Zsh) para indicar que el comando que sigue\n",
        "# debe ejecutarse en la shell ignorando cualquier configuración de tu entorno virtual actual."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gbc3m5DmkCc",
        "outputId": "45a319ba-0c4f-49d3-9d97-6fc774099732"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.5)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (4.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) Se hace la importación de las bibliotecas con las que vamos a trabajar\n",
        "\n",
        "from bs4 import BeautifulSoup     # Parseo (extracción de datos) de HTML\n",
        "import requests                   # Se hacen peticiones HTTP para descargar páginas web\n",
        "import re                         # Se utiliza para crear expresiones regulares (buscar patrones en el texto)\n",
        "import time                       # Sirve para hacer pausas entre las peticiones, esto sirve para evitar bloqueos\n",
        "\n",
        "from urllib.parse import urljoin  # Sirve para construir URLs absolutas desde rutas relativas, es decir, ayuda a establecer\n",
        "                                  # URLs para que requests pueda acceder a la páginación de las iniciativas del Congreso CDMX\n",
        "import pandas as pd               # Manejo de tablas (DataFrames) y para la exportación de CSV de la tabla con los dato del web scrapping\n"
      ],
      "metadata": {
        "id": "g51u9-q_nbyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) Configuración del Web Scraping\n",
        "\n",
        "BASE = \"https://ciudadana.congresocdmx.gob.mx\"           # Dominio base del sitio web en donde se hará el web scraping\n",
        "LIST_URL = f\"{BASE}/Iniciativa/iniciativas\"              # URl de la complementaria del sitio web donde se encuentran\n",
        "                                                         # las iniciativas del Congreso de la Ciudad de México\n",
        "                                                         # Se hace un f-string para poder insertar el valor {BASE}\n",
        "OUT_CSV = \"iniciativas_CDMX_2024_2025.csv\"               # Nombre del archivo de salida en formato CSV\n",
        "PAUSE = 0.4                                              # Pausa en segundos entre peticiones para evitar bloqueos del servidor\n",
        "\n",
        "session = requests.Session()                             # Crea una sesión HTTP reutilizable\n",
        "                                                         # Permite configurarlo \"una vez\" y  se aplica a todas las peticiones del scraping.\n",
        "session.headers.update({\"User-Agent\": \"Mozilla/5.0 (raspado academico cdmx)\"}) # Define User-Agent \"Amigable\"\n",
        "                                                                               # El \"User-Agent\" ayuda a que el servidor te reconozca como navegador “real”\n",
        "                                                                               # Algunos sitios bloquean agentes desconocidos\n",
        "req = session.get(LIST_URL, timeout=30)                  # Hacemos la solicitud para entrar a la página\n",
        "req.raise_for_status()                                   # Revisa el código HTTP de la respuesta y si es un error (4xx o 5xx)\n",
        "                                                         # lanza una excepción (requests.HTTPError) para evitar seguir trabajando con errores\n",
        "\n",
        "print(f\"Estado de la Solicitud: {req.status_code}\")      # Checa el estado de la solitud hacía la página, 200 indica que el intento fue éxitoso."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwc6-SX7qKTq",
        "outputId": "53b71e89-2580-45b3-bad8-6c465b022b17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estado de la Solicitud: 200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4) Web Scraping\n",
        "\n",
        "# Definimos la primera función para encontrar todos los resultados de la paginación del sitio web\n",
        "\n",
        "def get_total_pages(soap: BeautifulSoup) -> int:          # Define una función que recibe un objeto BeautifulSoup y\n",
        "                                                          # devuelve un número entero (int) para detectar cuantas páginas hay (1 - 46)\n",
        "                                                          # soap es el HTML de la página de listado ya parseado con BeautifulSoup\n",
        "    text = soap.get_text(\" \", strip=True)                 # Extrae todo el texto visible de la página\n",
        "                                                          # get_text(\" \", strip=True) convierte el HTML en una cadena “plana” y legible para regex\n",
        "    m = re.search(r\"Page\\s+\\d+\\s+of\\s+(\\d+)\", text, flags=re.I)  # Expresión regular que busca el patrón “Page 1 of X”\n",
        "                                                                 # flags=re.I es un parámetro de re.search/re.compile que no distigue mayúsculas\n",
        "                                                                 # de minúsculas al comparar, útil si queremos que nuestro patrón funcione en todos\n",
        "                                                                 # los caso (page, Page, PAGE, pAgE, PaGe, ETC.)\n",
        "    return int(m.group(1)) if m else 1                    # Devuelve X si existe; si no, asume 1 página para que el código no tenga error."
      ],
      "metadata": {
        "id": "ryPRDPi02cM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos la segunda función para normalizar texto extraído del HTML (evitar “basura” como múltiples espacios, tabs o saltos de línea).\n",
        "\n",
        "def _clean_text(x: str) -> str:                                 # Define una función que recibe un string y devuelve un string \"limpio\"\n",
        "    if x is None:                                               # Si el valor de entrada es None (no hay texto)\n",
        "        return None                                             # Devuelve None sin intentar procesarlo\n",
        "    return re.sub(r\"\\s+\", \" \", x.strip())                       # x.strip(): quita espacios/saltos al inicio y al final\n",
        "                                                                # re.sub(r\"\\s+\", \" \", ...): colapsa cualquier secuencia de espacios,\n",
        "                                                                # tabs o saltos de la linea en un solo espacio"
      ],
      "metadata": {
        "id": "DIqHgf6t7JwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos la tercera función para obtener información en los nodos como nombre y valor\n",
        "\n",
        "def extract_nombre_from_detail(detail_html: str):               # Define una función llamada extract_nombre_from_detail.\n",
        "                                                                # Recibe como parámetro detail_html (un string con HTML).\n",
        "    \"\"\"\n",
        "    Extrae SOLO el valor de 'Nombre:' de la ficha. (Sin usar h1/h2 para no confundir con el promovente y evitar falsos positivos porque en muchas fichas esos encabezados traen el promovente, no el título de la iniciativa.\n",
        ")\n",
        "    Soporta: <dt>Nombre</dt><dd>VALOR</dd> y <b>Nombre:</b> VALOR\n",
        "\n",
        "    Dos formatos principales:\n",
        "\n",
        "Definición (<dt>/<dd>): patrón muy común en fichas. Se toma el <dd> que sigue al <dt>Nombre</dt>.\n",
        "\n",
        "Inline (negritas u otros): <b>Nombre:</b> Valor. Se leen los next_siblings hasta topar un separador (<br>, <hr>) o un nuevo <dt>.\n",
        "    \"\"\"\n",
        "\n",
        "    s = BeautifulSoup(detail_html, \"html.parser\")                        # Parsea el HTML en un árbol navegable\n",
        "                                                                         # s es el objeto raíz que permite buscar nodos, recorrer hijos, etc.\n",
        "    label = s.find(                                                      # Busca una etiqueta (<b>, <strong>, <span>, <label> o <dt>).\n",
        "                                                                         # Filtra por el texto que empiece con \"nombre\".\n",
        "                                                                             # label será la etiqueta que contiene el título del campo (“nombre”).\n",
        "        lambda tag: tag.name in (\"b\", \"strong\", \"span\", \"label\", \"dt\")   # lambda tag: es una función anónima que recibe cada tag de HTML en donde se revisan 2 condiciones\n",
        "                                                                         # 1) Que la etiqueta se llame <b>, <strong>, <span>, <label> o <dt>.\n",
        "                                                                         # 2) Que el texto que contiene empiece con \"nombre\" (ignorando mayúsculas y espacios).\n",
        "\n",
        "        and tag.get_text(strip=True).lower().startswith(\"nombre\"))       # .get_text es un método de BeautifulSoup que extrae todo el contenido dentro de una etiqueta HTML, ignorando etiquetas hijas\n",
        "                                                                         # Strip=True es un parámetro que quita espacios en blanco al inicio y al final del texto, además, colapsa espacios extra\n",
        "                                                                         # .lower() es un método de strings de Python que convierte todo el texto a minúsculas, se usa para que la comparación\n",
        "                                                                         # no dependa de mayúsculas o minúsculas\n",
        "                                                                         # .startswith(\"nombre\"): Es un método de strings en Python,\n",
        "                                                                         # que pregunta si una cadena empieza con cierto prefijo\n",
        "                                                                         # Significa: “¿el texto del nodo comienza con la palabra nombre?”\n",
        "\n",
        "    if label:                                                            # Si se encontró la etiqueta del campo \"nombre\"\n",
        "        if label.name ==\"dt\":                                            # find_next(\"\") es un método de BeautifulSoup que busca el siguiente nodo en el árbol HTML que cumpla con. el selector (\"dd\")\n",
        "                                                                         # Caso 1: Formato de definición <dt>Nombre</dt><dd>Valor</dd>\n",
        "            dd = label.find_next(\"dd\")                                   # Toma el siguiente valor <dd> (El valor emparejado)\n",
        "            if dd:\n",
        "                val = _clean_text(dd.get_text(\" \", strip=True))          # _clean_text(): función auxiliar que limpia espacios dobles, saltos de línea, etc.\n",
        "                                                                         # Si encuentra el valor (val), lo devuelve.\n",
        "                if val:\n",
        "                    return val                                           # Devuelve el valor si exite éxito\n",
        "        parts = []                                                       # Caso 2: Formato inline: <b>Nombre:</b> Valor (u otros tags)\n",
        "        for sib in label.next_siblings:                                  # Recorre los hermanos que vienen después del label\n",
        "            if getattr(sib, \"name\", None) in (\"br\", \"hr\", \"dt\"):         # Si aparece un separador o nuevo <dt>, cortamos\n",
        "                break                                                    # hasattr() es una función incorporada de Python que responde:\n",
        "                                                                         # ¿el objeto \"x\" tiene un atributo llamado attr? Si sí, devuelve True.\n",
        "                                                                         # getattr() además de verificar, lo obtiene (o devuelve default si no existe).\n",
        "            txt = sib.get_text(\" \", strip=True) if hasattr(sib, \"get_text\") else str(sib)  # Sirve para obtener texto hermano\n",
        "            txt = _clean_text(txt)                                       # Limpia los espacios y los saltos\n",
        "            if txt:\n",
        "                parts.append(txt)                                        # Acumula los fragmentos del valor\n",
        "        lab_txt = _clean_text(label.get_text(\" \", strip=True))           # Revisa si el label contiene \"Nombre: VALOR\"\n",
        "        if lab_txt and \":\" in lab_txt:                                   # Si el propio label trae \"Nombre: Algo\"\n",
        "            after = lab_txt.split(\":\", 1)[1].strip()                     # Toma lo que viene despues de los dos puntos\n",
        "            if after:\n",
        "                parts.insert(0, after)                                   # Inserta primero para que quede antes que los siblings\n",
        "        val = _clean_text(\" \".join(parts)).strip(\":\") if parts else None # Une y limpia posible \":\" residuales\n",
        "        if val:\n",
        "            return val                                                   # Devuelve el valor obtenido en formato inline\n",
        "    full = s.get_text(\"\\n\", strip=True)                                # Fallback: toma TODO el texto plano de la ficha (con saltos)\n",
        "    m= re.search(r\"^\\s*Nombre\\s*:\\s*(.+)$\", full, flags=re.I | re.M)     # Busca una línea que empiece con \"Nombre:\"\n",
        "    if m:\n",
        "        return _clean_text(m.group(1))                                   # Devuelve el valor\n",
        "    return None                                                          # Si no se encontró, devuelve el valor None"
      ],
      "metadata": {
        "id": "mPqxT-h1-OgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos la cuarta función para  extraer información del enlace \"Ver más\" (nombre y partido político)\n",
        "\n",
        "def pull_card_context(anchor):                                           # Extrae datos del \"card\" (lista) alrededor del enlace \"Ver más\"\n",
        "                                                                         # anchor es un nodo del DOM (árbol HTML), en este caso tiene el <a> (Ver más)\n",
        "    el = anchor                                                          # Comienza desde el enlace de \"Ver más\"\n",
        "    block_text = \"\"                                                      # Inicializa contenedor de texto de tarjeta\n",
        "    for _ in range(6):                                                   # Sube hasta el nivel 6 del árbol DOM\n",
        "        if not el:\n",
        "            break\n",
        "        block_text =el.get_text(\"\\n\", strip=True)                        # Toma el texto del nodo actual\n",
        "        if \"Suscrita por\" in block_text:                                 # Si contiene \"Suscrita por\", ya es el bloque deseado\n",
        "            break\n",
        "        el = el.parent                                                   # Si no, sigue subiendo al padre\n",
        "    m = re.search(r\"Suscrita por:\\s*(.*?)\\s*\\|\\s*(.*?)\\s*\\|\", block_text, flags=re.I)   # Expresión regular que captura NOMBRE y PARTIDO/ROL\n",
        "    if m:\n",
        "        suscriptor_nombre = _clean_text(m.group(1))                      # Limpia y toma el primer grupo (nombre)\n",
        "        suscriptor_partido_rol = _clean_text(m.group(2))                 # Limpia y toma el segundo grupo (partido/institución/rol)\n",
        "    else:\n",
        "        suscriptor_nombre = None                                         # Si no se encontró, deja None\n",
        "        suscriptor_partido_rol = None\n",
        "    return suscriptor_nombre, suscriptor_partido_rol                     # Devuelve una tupla con (nombre, partido/rol)"
      ],
      "metadata": {
        "id": "RsZ31JJgJ-3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5) Title case español con conectores en minúscula para mantener en minúscula ciertas palabras de enlace (artículos, preposiciones, conjunciones) salvo si son la primera o última palabra\n",
        "\n",
        "# Definimos la primera funcion para formatear el texto a “Title Case” español (con stopwords en minúscula)\n",
        "\n",
        "def word_title_text(s: str, keep_acronyms=None) -> str:                  # Formatea texto a “Title Case” español (con stopwords en minúscula)\n",
        "  \"\"\"\n",
        "    Title case por palabra con 'stopwords' en minúscula (salvo si son la primera o la última palabra).\n",
        "    - Reemplaza '_' por espacio y colapsa espacios.\n",
        "    - Pasa todo a minúsculas y capitaliza palabra a palabra.\n",
        "    - Mantiene en minúscula conectores/artículos/preps (o, y, e, u, de, del, de la, la, el, los, las, a, al, en, con, por, para, sin, sobre, entre, hacia, hasta, según, tras, como, que).\n",
        "    - Preserva acrónimos dados en `keep_acronyms` (p. ej., {\"CDMX\",\"UNAM\"}).\n",
        "    \"\"\"\n",
        "  if pd.isna(s) or not isinstance(s, str):                             # pd.isna(s) (Pandas): devuelve True si s es NaN, None, NaT, o equivalente de pandas. Protege contra valores faltantes\n",
        "                                                                       # instance(s, str): comprueba que s sea un str\n",
        "    return s                                                           # Si es NaN o no es string, regresa tal cual\n",
        "  stopwords = {                                                        # Se crea un set con los acrónimos que queremos preservar tal cual en minúsculas\n",
        "      \"a\",\"al\",\"ante\",\"bajo\",\"cabe\",\"con\",\"contra\",\"de\",\"del\",\"desde\",\"en\",\"entre\",\"hacia\",\n",
        "      \"hasta\",\"para\",\"por\",\"según\",\"sin\",\"so\",\"sobre\",\"tras\",\"y\",\"e\",\"o\",\"u\",\"la\",\"el\",\"los\",\n",
        "      \"las\",\"un\",\"una\",\"unos\",\"unas\",\"que\",\"como\",\"lo\",\"del\",\"de\",\"dela\",\"dell\",\"dela\"}\n",
        "  keep = set(keep_acronyms or [])                                      # Conjunto de acrónimos a preservar\n",
        "  s2 = re.sub(r\"_+\", \" \", s)                                           # Sustituye uno o varios _ contiguos por un único espacio\n",
        "  s2 = re.sub(r\"\\s+\", \" \", s2.strip()).lower()                         # Reduce múltiples espacios a uno solo y pasa a minúsculas\n",
        "  if not s2:                                                           # Si no hay texto, regresa tal cual\n",
        "    return s2\n",
        "  words = s2.split(\" \")                                                # Divide en palabras\n",
        "  n = len(words)                                                       # Número de palabras del objeto words que se guarda en el objeto n\n",
        "  out = []                                                             # Acumulará palabras formateadas\n",
        "  for i, w in enumerate(words):                                        # Itera cada palabra y su posición, es decir, se recorre cada palabra w con su índice i.\n",
        "    word = w                                                           # Regla: si es la primera palabra (i==0) o la última (i==n-1)\n",
        "                                                                       # o la palabra NO es una stopword, se capitaliza la primera letra: w[:1].upper() + w[1:].\n",
        "                                                                       # Por defecto, deja la palabra en minúscula\n",
        "    if i == 0 or i == n - 1 or w not in stopwords:                     # Capitaliza si es 1ª/última o no es stopword\n",
        "                                                                       # Ej: \"ley de tránsito\" → [\"Ley\", \"de\", \"Tránsito\"].\n",
        "                                                                       # Si la palabra es stopword y está en medio, se deja en minúscula.\n",
        "      word = w[:1].upper() + w[1:]\n",
        "    out.append(word)                                                   # Agrega la palabra resultante\n",
        "  result = \" \".join(out)                                               # Finalmente se reconstruye la cadena con \" \".join(out).\n",
        "\n",
        "  if keep:                                                             # Si hay acrónimos a preservar (p. ej., CDMX)\n",
        "    tokens = result.split(\" \")                                         # Vuelve a dividir por palabra\n",
        "    tokens = [\n",
        "        t if t.lower() not in {k.lower() for k in keep}\n",
        "        else next(iter({k for k in keep if k.lower()==t.lower()}))\n",
        "        for t in tokens]\n",
        "    result = \" \".join(tokens)\n",
        "  return result                                                        # Devuelve el texto en “title case” español"
      ],
      "metadata": {
        "id": "H_6raAfrXSZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos la segunda función para renombrar las columns del archivo CSV que se va a exportar al final\n",
        "\n",
        "def normalize_headers_with_mapping(df: pd.DataFrame) -> pd.DataFrame:  # Establece nombres finales de columnas\n",
        "    \"\"\"\n",
        "    Renombra columnas al catálogo final y aplica word_title_text a cualquier otra columna que aparezca.\n",
        "    \"\"\"\n",
        "    rename_map = {                                                     # Mapa de nombres internos a nombres finales\n",
        "        \"nombre_iniciativa\": \"Nombre de la Iniciativa\",\n",
        "        \"suscriptor_nombre\": \"Nombre del Suscriptor o Suscriptora\",\n",
        "        \"suscriptor_partido_rol\": \"Partido Político, Institución o Asociación\",\n",
        "        \"url\": \"Url\",\n",
        "    }\n",
        "    df = df.rename(columns={k: v for k, v in rename_map.items() if k in df.columns})      # Aplica renombrado donde aplique\n",
        "    df.columns = [word_title_text(c, keep_acronyms={\"CDMX\",\"UNAM\"}) for c in df.columns]  # Aplica “title case” español\n",
        "    return df                                                                             # Devuelve el DataFrame con encabezados finales\n"
      ],
      "metadata": {
        "id": "q1SBv9EfXWIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos la tercera función para terminar de formatear el archivo CSV que se exporta al final\n",
        "\n",
        "\n",
        "def normalize_dataframe_content(df: pd.DataFrame, url_col_name: str = \"Url\") -> pd.DataFrame:  # Formatea contenido\n",
        "    \"\"\"\n",
        "    Aplica word_title_text al contenido de columnas de texto (excepto Url).\n",
        "    \"\"\"\n",
        "    df = df.copy()                                                                             # Trabaja sobre una copia\n",
        "    for col in df.select_dtypes(include=\"object\").columns:                                     # Recorre columnas de tipo texto\n",
        "        if col == url_col_name:                                                                # Salta la columna Url (no tocar enlaces)\n",
        "            continue\n",
        "        df[col] = df[col].apply(lambda x: word_title_text(x, keep_acronyms={\"CDMX\",\"UNAM\"}))   # Aplica formateo\n",
        "    return df                                                                                  # Devuelve el DataFrame formateado"
      ],
      "metadata": {
        "id": "dCm71_d1FLHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6) Proceso principal\n",
        "def main():                                               # Función principal del script\n",
        "    r = session.get(LIST_URL, timeout=30)                 # Solicita la página de lista (desactiva verificación SSL)\n",
        "    r.raise_for_status()                                  # Lanza error si la respuesta HTTP no fue exitosa\n",
        "    soup = BeautifulSoup(r.text, \"html.parser\")           # Parsea el HTML de la lista\n",
        "    total_pages = get_total_pages(soup)                   # Detecta cuántas páginas hay en total\n",
        "    print(f\"[INFO] Total de páginas detectadas: {total_pages}\")  # Log informativo\n",
        "\n",
        "    rows, seen_detail = [], set()                         # Inicializa acumulador de filas y set de detalles ya vistos\n",
        "\n",
        "    for p in range(1, total_pages + 1):                   # Itera cada página del paginador\n",
        "        url = LIST_URL if p == 1 else f\"{BASE}/Iniciativa/iniciativas?page={p}\"  # Arma URL de la página p\n",
        "        print(f\"[INFO] Página {p}/{total_pages}: {url}\")  # Log de progreso\n",
        "        resp = session.get(url, timeout=30)               # Descarga el HTML de la página p\n",
        "        resp.raise_for_status()                           # Verifica éxito de la solicitud\n",
        "        s = BeautifulSoup(resp.text, \"html.parser\")       # Parsea HTML de la página p\n",
        "\n",
        "        anchors = s.find_all(\"a\", string=re.compile(r\"ver más\", re.I))  # Busca todos los enlaces “Ver más”\n",
        "        for a in anchors:                                 # Itera cada tarjeta/enlace encontrado\n",
        "            href = a.get(\"href\")                          # Obtiene href del enlace\n",
        "            if not href:\n",
        "                continue                                  # Si no hay href, salta\n",
        "            detail_url = urljoin(BASE, href)              # Convierte href relativo en URL absoluta\n",
        "            if detail_url in seen_detail:                 # Evita procesar el mismo detalle dos veces\n",
        "                continue\n",
        "            seen_detail.add(detail_url)                   # Marca el detalle como visto\n",
        "\n",
        "            suscriptor_nombre, suscriptor_partido_rol = pull_card_context(a)  # Extrae datos del card (lista)\n",
        "\n",
        "            d = session.get(detail_url, timeout=30)       # Solicita la página de detalle\n",
        "            d.raise_for_status()                          # Verifica éxito\n",
        "            nombre_iniciativa = extract_nombre_from_detail(d.text)  # Extrae “Nombre:” de la ficha\n",
        "\n",
        "            rows.append({                                 # Agrega una fila al resultado\n",
        "                \"nombre_iniciativa\": nombre_iniciativa,\n",
        "                \"suscriptor_nombre\": suscriptor_nombre,\n",
        "                \"suscriptor_partido_rol\": suscriptor_partido_rol,\n",
        "                \"url\": detail_url,\n",
        "            })\n",
        "            time.sleep(PAUSE)                             # Pausa corta entre detalles\n",
        "        time.sleep(PAUSE)                                 # Pausa corta entre páginas\n",
        "\n",
        "    df = pd.DataFrame(rows)                               # Crea DataFrame con todas las filas extraídas\n",
        "\n",
        "    df = normalize_headers_with_mapping(df)               # Aplica nombres finales de columnas (title case español)\n",
        "    df = normalize_dataframe_content(df, url_col_name=\"Url\")  # Formatea contenido (excepto Url)\n",
        "\n",
        "    df.to_csv(OUT_CSV, index=False, encoding=\"utf-8\")     # Exporta el DataFrame a CSV\n",
        "    print(f\"[OK] CSV generado: {OUT_CSV}  ({len(df)} filas)\")  # Mensaje final con ruta y número de filas\n",
        "\n",
        "if __name__ == \"__main__\":                                # Punto de entrada del script\n",
        "    main()                                                # Ejecuta la función principal\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "6Y-ED_wnGSa5",
        "outputId": "28cb5e04-d264-49b7-b51d-c2015cf8509f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Total de páginas detectadas: 46\n",
            "[INFO] Página 1/46: https://ciudadana.congresocdmx.gob.mx/Iniciativa/iniciativas\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2620705505.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m                                \u001b[0;31m# Punto de entrada del script\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                                                \u001b[0;31m# Ejecuta la función principal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-2620705505.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0;34m\"url\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdetail_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             })\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPAUSE\u001b[0m\u001b[0;34m)\u001b[0m                             \u001b[0;31m# Pausa corta entre detalles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPAUSE\u001b[0m\u001b[0;34m)\u001b[0m                                 \u001b[0;31m# Pausa corta entre páginas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Nota final:**\n",
        "\n",
        "En este proyecto se aplicaron técnicas vistas en el módulo, como expresiones regulares para identificar y extraer patrones de texto (p. ej., “Page 1 of X” o “Nombre:”); funciones y funciones lambda para encapsular y limpiar la lógica de parsing; ciclos for in range para recorrer la paginación y los enlaces de cada página; estructuras if/else para controlar el flujo ante casos faltantes o variantes del HTML; y métodos de listas como `.append()`para ir acumulando los registros obtenidos.\n",
        "\n",
        "Finalmente, se utilizó pandas para estructurar la información en un DataFrame y exportarla a CSV, garantizando encabezados y contenido con el formato solicitado.\n"
      ],
      "metadata": {
        "id": "Xvk2XreqKuZm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vCODE39yCRix"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}